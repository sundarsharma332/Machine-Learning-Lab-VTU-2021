1.
import csv
with open('finds.csv' , 'r') as any:
    reader = csv.reader(any)
    data = []
    for row in reader:
        data.append(row)        

h = ['ϕ','ϕ','ϕ','ϕ','ϕ','ϕ']
print("Begin Hypothesis :" , h)
for row in data:
    if row[-1] == 'y':
        print("Training data : " , row)
        j = 0
        for col in row:
            if col != 'y':
                if col != h[j] and h[j] == 'ϕ':
                    h[j] = col
                elif col != h[j] and h[j] != 'ϕ':
                    h[j] = '?'
            j = j + 1
                    
        print("updated Hypothesis : " ,h)    
        print()
print('Maximally specefic hypothesis:' , h)

2.

import csv
#open the datasets and push the rows in the data list
with open('candidate.csv' , 'r') as any:
    reader = csv.reader(any)
    data = [] #creating the data list
    
    for i in reader:
        data.append(i) #append the data
        
L = len(data[0]) - 1    #length of the row #initially 6
S = ['phi'] * L
G = ['?'] * L
H = []
for i in range(0 , L):
    S[i] = data[1][i]
for i in range(1 , len(data)):
    if data[i][L] == 'yes':
        for j in range(0 , L):
            if(S[j] != data[i][j]):
                S[j] = '?'
        for j in range(0 , L):
            for k in range(0 , len(H)):
                if H[k][j] != S[j] and H[k][j] !='?':
                    del H[k]
    if data[i][L] == 'no':
        for j in range(0 , L):
            if data[i][j] != S[j] and S[j] != '?':
                G[j] = S[j]
                H.append(G)
                G = ['?'] * L
print("general Hypothesis" ,H)    
print("specefic_h" , S)

4.
import numpy as np
ip = np.array(([2 , 9] , [1, 5], [3 , 6]), dtype = float)
op = np.array(([92] , [86] , [89]) , dtype = float)
ip = ip / np.amax(op , axis=0)
op = op/ 100;
print(op)
print(ip)
class NeuralNetwork(object):
    
    def __init__(self):
        self.i_s = 2
        self.o_s = 1
        self.h_s = 3

        self.w1 = np.random.randn(self.i_s, self.h_s)
        self.w2 = np.random.randn(self.h_s, self.o_s)
            
    def feedForward(self, ip):
        self.z1 = np.dot(ip, self.w1)
        self.z2 = self.sigmoid(self.z1)
        self.z3 = np.dot(self.z2, self.w2)
        predection = self.sigmoid(self.z3)
        return predection  
        
    def sigmoid(self, s,deriv=False):
            if (deriv == True):
                return s * (1 - s)
            return 1 / (1 + np.exp(-s))
    
    def backward(self , ip , op , pred):
        self.pred_error = op - pred
        self.pred_delta = self.pred_error * self.sigmoid(pred , deriv = True)

        self.z2_error = self.pred_delta.dot(self.w2.T)
        self.z2_delta = self.z2_error * self.sigmoid(self.z2 ,deriv = True)

        self.w1 += ip.T.dot(self.z2_delta)
        self.w2 += self.z2.T.dot(self.pred_delta)
            
    def train(self , ip , op):
        pred = self.feedForward(ip)
        self.backward(ip , op , pred)    
    
NN = NeuralNetwork()
for i in range(1): #trains the NN 1000 times
    NN.train(ip, op)
        
print("Input: " + str(ip))
print("Actual Output: " + str(op))
print("Loss: " + str(np.mean(np.square(op - NN.feedForward(ip)))))
print("\n")
print("Predicted Output: " + str(NN.feedForward(ip)))        


6.Naive classifier
import pandas as pd 
msg=pd.read_csv('naiveclassifier.csv',names=['message','label'])
print('The dimension of the dataset',msg.shape)
msg['labelnum']=msg.label.map({'pos':1,'neg':0})
X=msg.message
y=msg.labelnum
print(X)
print(y)
from sklearn.model_selection import train_test_split
xtrain,xtest,ytrain,ytest=train_test_split(X,y)
print(xtest.shape)
print(xtrain.shape)
print(ytest.shape)
print(ytrain.shape)
from sklearn.feature_extraction.text import CountVectorizer
count_vect=CountVectorizer()
xtrain_dtm=count_vect.fit_transform(xtrain)
xtest_dtm=count_vect.transform(xtest)
print(count_vect.get_feature_names())
df=pd.DataFrame(xtrain_dtm.toarray(),columns=count_vect.get_feature_names())
print(df)
print(xtrain_dtm)
from sklearn.naive_bayes import MultinomialNB
df=MultinomialNB().fit(xtrain_dtm,ytrain)
predicted=df.predict(xtest_dtm)
from sklearn import metrics
print('Accuracy Metrics')
print('Accuracy of the classifier is',metrics.accuracy_score(ytest,predicted))
print('Confusion matrix')
print(metrics.confusion_matrix(ytest,predicted))
print('Recall and Precision')
print(metrics.recall_score(ytest,predicted))
print(metrics.precision_score(ytest,predicted))

8.Kmeans

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn import datasets
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score
iris = datasets.load_iris()
X = pd.DataFrame(iris.data)
X.columns = ['Sepal_Length', 'Sepal_Width', 'Petal_Length', 'Petal_Width']
y = pd.DataFrame(iris.target)
y.columns = ['Targets']
kmeans = KMeans(n_clusters=3)
clusters = kmeans.fit_predict(X)
from scipy.stats import mode
labels = np.zeros_like(clusters)
for i in range(3):
    cat = (clusters == i)
    labels[cat] = mode(iris.target[cat])[0]
acc = accuracy_score(iris.target, labels)
print('Accuracy = ', acc)

plt.figure(figsize=(10, 10))
colormap = np.array(['red', 'lime', 'blue'])

# Plot the Original Classifications using Petal features
plt.subplot(2, 2, 1)
plt.scatter(X.Petal_Length, X.Petal_Width, c = colormap[y.Targets], s=40)
plt.title('Real Clusters')
plt.xlabel('Petal Length')
plt.ylabel('Petal Width')

# Plot KMeans Model Classifications
plt.subplot(2, 2, 2)
plt.scatter(X.Petal_Length, X.Petal_Width, c = colormap[labels], s = 40)
plt.title('KMeans Clusters')
plt.xlabel('Petal Length')
plt.ylabel('Petal Width')

# General EM for GMM
from sklearn import preprocessing

# transform data such that the distribution mean = 0 and std = 1
scaler = preprocessing.StandardScaler()
scaler.fit(X)
scaled_X = scaler.transform(X)

xs = pd.DataFrame(scaled_X, columns = X.columns)

from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=3)
gmm_y = gmm.fit_predict(xs)

labels = np.zeros_like(clusters)

for i in range(3):
    cat = (gmm_y == i)
    labels[cat] = mode(iris.target[cat])[0]
    
acc = accuracy_score(iris.target, labels)
print("Accuracy using GMM = ", acc)

plt.subplot(2, 2, 3)
plt.scatter(X.Petal_Length, X.Petal_Width, c = colormap[gmm_y], s = 40)
plt.subplots_adjust(hspace=0.4, wspace=0.4)
plt.title('GMM Clusters')
plt.xlabel('Petal Length')
plt.ylabel('Petal Width')

9.k nearest 

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report,confusion_matrix
from sklearn import datasets
iris = datasets.load_iris()
iris_data = iris.data;
iris_labels = iris.target
print(iris_labels)
x_train,x_test,y_train,y_test=train_test_split(iris_data,iris_labels,test_size=0.30)
classifier=KNeighborsClassifier(n_neighbors=5)
classifier.fit(x_train,y_train)
y_pred=classifier.predict(x_test)
print("confusion Matrics is:")
print(confusion_matrix(y_test , y_pred))
print("Accuracy Matrix is:")
print(classification_report(y_test , y_pred))